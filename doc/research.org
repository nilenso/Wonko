* Research and Spike notes

** Goals for initial release
    - simple interface for all sparx services
    - monitor node level metrics: resource exhaustion (cpu spike, i/o spike, disk full)
    - monitor low volume application level metrics: krikkit feed processed with error summary
    - alerting: machine went down, queue full, feed not present, unable to parse feed, upload to s3 failed
    - thresholds for alerts (example: 1st, 10th, 100th errors should be alerted)
    - integration with pager-duty
    - nice to have: graphs

** Assumptions (for initial release)
    - low volume: not real time requests
    - log-like interface from applications (log/monitor from anywhere in the codebase with one line)
    - no configuration in wonko (alert configs, etc would be within prometheus, or another solution)

** Spike goals:
    - what (if anything) to place between wonko and krikkit
    - should we use prometheus or another monitoring solution, or build it inside wonko?
    - data format to use between wonko and krikkit - with schema or schemaless?
    - find out if we need to restrict the "type" of metrics wonko will accept - like prometheus' counter/gauge/histogram, etc
    - node_exporter / other existing solutions / build it ourselves

** Example list of metrics to monitor from krikkit
    - feed not present
    - unable to parse feed
    - upload to s3 failed
    - number of successes/errors after parsing a feed

** Notes on the kind of monitoring krikkit needs

Pasting Pavan's notes from slack:

There will be two kinds of things that we need in Krikkit:
1) Current Status (good or bad) - This will help us find out when
   things go down - nodes, feeds, jobs etc.
2) Values over time - This will help us when things start degrading -
   Feed processing time, memory usage etc. On the node level,
   available disk space etc.

In Krikkit, there are also interesting threshold kind of metrics as
well. For example:
- If the COGS of any SKU goes below a certain threshold, notify
- If the number of SKUs from a given feed is off than a median # of
  SKUs for that feed, notify

So, the idea is since Krikkit can end up getting a lot of jobs and
feeds, baby sitting it becomes a full time job. Instead, we should be
able to know when things go wrong. Of course, it wont be exhaustive
from day 1, but as we go along maintaining it, if its easy to start
monitoring things that we usually do manually, our purpose with Wonko
will be served.

** Day wise notes
*** srihari-2016-02-04
**** On using kafka as a messaging system between applications and Wonko
***** Implied high level design
#+begin_src
+---------+                              Kafka consumer       +----------+
| krikkit | +----------> +-------+         +-------+------>   |prometheus|
+---------+              | kafka +-------> | wonko |          +----------+
   Kafka producers       +-------+         +-------+            OR
+------------+             ^                       |
| eccentrica |-------------+                       |      +------------------+
+------------+                                     +----> | riemann|graphite |
                                                          +------------------+
#+end_src

***** Positives
- Kafka will give us the high throughput we need when applications
  send their real time events (~5k requests per second).
- Producers (applications) will need to implement kafka producers
  for sending events/logs, and the interface will be simple.
- We could even just send logs, and stream-process it using Storm
  or Flume to events for monitoring.
- Kafka is amazing

***** Negatives
+ Understanding kafka, operational overhead, instrumenting kafka and zookeeper
+ We probably won't use the stream processing capability of kafka for monitoring
+ The durability aspect isn't very important for monitoring
  metrics because we're probably going to use a time series
  database (graphite or prometheus) for monitoring graphs.
+ We only have one consumer -> Wonko.

***** Resources
- https://dzone.com/articles/reporting-metrics-apache-kafka
- https://github.com/stealthly/metrics-kafka
- http://blog.mmlac.com/log-transport-with-apache-kafka/
- https://github.com/pingles/clj-kafka

***** Conclusions
- Kafka would be a good message buffer to solve the problem of high
  volume of requests, but from the features it provides, it feels like
  we'll be under-utilizing it, which hints at a different solution.
- For the low volume release (krikkit), it seems quite
  unnecessary. However, the interface for applications might change
  once we plug in kafka, so we should consider using it from the get
  go.
- If we are serious about log monitoring then it might be a good
  solution to keep logs in one place (kafka), process them (using
  Storm or Flume) for debugging or other insights besides monitoring.

**** Should we use prometheus or another monitoring solution, or build it inside wonko?
- Wonko would be the interface for applications, so the initial choice shouldn't matter much.
    - We should however pick an existing solution before debating
      writing something within wonko, so that Wonko can be used.
- For the first release at least, I think we should Prometheus.
    - We have used prometheus/grafana for a while now to monitor node level metrics, and it works.
    - We have already built integration with prometheus for 3 projects (HK, EP, Hatter).
**** Wonko API
- This is regarding the shape of requests/api calls that services like
  krikkit and EP will make to wonko. I see the following alternatives
  (ordered by priority).

  1. Metrics
  2. Logs with schema
  2. Logs without schema

***** Metrics
For example, krikkit/EP would have the following along with the log lines:
#+begin_src
;; metric-type metric-name map-of-labels-and-values options

(monitor :counter :cogs-job-completed)
(monitor :counter :no-new-surise-feed-found {:alert true})
(monitor :gauge :cogs-job-stats {:successes 107 :errors 3 :exec-time 42})
(monitor :histogram :get-buckets {:status 200} 42)
#+end_src

Notes:
- In wonko, we'd have to dynamically create these metrics to send to
  prometheus, riemann, etc. If not dynamically, applications will have
  to register these types with Wonko first, but this would involve
  additional complexity in the service<=>wonko interaction.
- Metric types are mandatory. This means that devs will have to
  understand these metric types.
- This is sort of what we were working towards with EP monitoring ([3]
  and [4]).
- These may or may not be tied directly with the logging, because it
  involves chosing the metric type.
- Injesting this data into wonko would be simple.

Resources:
  1 https://dropwizard.github.io/metrics/3.1.0/getting-started
  2 https://prometheus.io/docs/concepts/metric_types/
  3 https://github.com/StaplesLabs/Eccentrica/blob/master/src/eccentrica/utils/monitoring.clj
  4 https://github.com/StaplesLabs/Eccentrica/blob/master/src/eccentrica/monitoring.clj

***** Logs with schema
For example, krikkit/EP would have the following log lines:
#+begin_src
;; log/log-level metric-name metric-info-map

(log/info :cogs-job-completed)
(log/error :no-new-surise-feed-found)
(log/info :cogs-job-stats {:successes 107 :errors 3 :exec-time 42})
(log/info :get-buckets {:status 200 :exec-time 42})
#+end_src

Notes:
- Without the metric type, we can't dynamically find out how to monitor a metric.
- We could alert based on a configured log-level
- Alternatively, we could code the transformation of these metrics
  inside wonko, which would be very similar to the "without schema"
  solution in disadvantages.

***** Logs without schema
- We'll have to write app specific parsers within Wonko. These parsers
  might have be versioned.
- This would be quite similar to what we see in Vogon, where we
  hand-write the transformation for each source. This would probably
  imply high maintenance.
- Using storm/flume to analyze logs inside kafka seems to be a common
  use case for complex queries around error information or when we
  need to correlate information across services. But not for regular
  health monitoring and alerting.
***** Other
We could also explore a combination of the above mechanisms.
**** Existing solutions for exporting node level metrics
A lot of existing monitoring tools have their own system resource
monitoring exporters. There aren't many (any) generic exporters out
there. Here are a few that might be useful when we build our standard
exporter for wonko. My preference would be to fork prometheus'
node_exporter written in go and make it generic.

- https://github.com/zcaudate/sigmund
- https://github.com/aphyr/riemann-tools/blob/master/bin/riemann-health
- https://github.com/ganglia/monitor-core/tree/master/gmond/modules
- https://github.com/prometheus/node_exporter
